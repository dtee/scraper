//
// Attention: Generated code! Do not modify by hand!
// Generated by: HibernateEntityImpl.vsl in andromda-hibernate-cartridge.
//
package com.ten45.entity.aggregation;

import java.net.URL;
import java.util.Collections;
import java.util.HashMap;
import java.util.LinkedList;
import java.util.List;

import com.ten45.basic.Log;
import com.ten45.basic.util.IOUtil;
import com.ten45.domain.crawler.Group;
import com.ten45.domain.crawler.Site;
import com.ten45.entity.product.ProductCategory;
import com.ten45.manager.aggregator.CrawlManagerImpl;

/**
 * @see com.ten45.entity.aggregation.CrawlSession
 */
public class CrawlSessionImpl extends com.ten45.entity.aggregation.CrawlSession
{
    /**
     * The serial version UID of this class. Needed for serialization.
     */
    private static final long       serialVersionUID = 4422859926869962819L;
    private Site                    site;
    private HashMap<String, Group>  groupMap;               // Used to find the group in the site easily
    private boolean                 isRunning;              // Used for crawl task, crawling or not.
    private String                  debugString;            // Used for debugging, print category tree + Product to string
    private String                  phase;                  // The current phase of the session.
    private boolean                 isAbort;                // Abort signal send by by crawl task.
    private HashMap<String, CrawlElement> urlHash;             // Url hash to store product url information.

    public CrawlSessionImpl ()
    {
        this.groupMap = new HashMap<String, Group>();
        this.urlHash = new HashMap <String, CrawlElement>();
    }
    
    
    /**
     * Load product elements and start up a product details crawl process   
     *  - Loads them by calling OnlineProductDao
     */
    public void loadProductElements() {
        
    }
    
    /***
     * Load category url and start up a category crawling process
     *  - Loads them by calling productCategoryDao
     */
    public void loadCategoryElements() {
        
    }
    
    private String crawlTypeString;
    
    /**
     * Over rides setCrawltype to set the crawltype string
     */
    public void setCrawlType(Integer crawlType) {
        // Handle the null case
        if (crawlType == null) {
            crawlType = CrawlManagerImpl.CRAWL_TYPE_CATEGORY;
        }
        
        super.setCrawlType(crawlType);
        
        if (crawlType == CrawlManagerImpl.CRAWL_TYPE_CATEGORY) {
            crawlTypeString = "Category";            
        }
        else if (crawlType == CrawlManagerImpl.CRAWL_TYPE_PRODUCTDETAILS) {
            crawlTypeString = "Product Details";            
        }
        else if (crawlType == CrawlManagerImpl.CRAWL_TYPE_PRODUCTS) {
            crawlTypeString = "Products";            
        }
    }
    
    public String getCrawlTypeString() {
        return crawlTypeString;
    }
    
    public void loadSiteElements() {
        this.Reset();
    }
    
    /**
     * Load url hash from the toCrawlElemnt and crawled element list
     * - This function need to be called after the crawl session is loaded from database.
     *
     */
    public void loadUrlHash() {
        this.urlHash = new HashMap <String, CrawlElement>();
        
        for (Object obj: super.getToCrawlElements()) {
            CrawlElement crawlElement = (CrawlElement) obj;
            this.urlHash.put(crawlElement.getUrl().toString(), crawlElement);
        }
        
        for (Object obj: super.getCrawledElements()) {
            CrawlElement crawlElement = (CrawlElement) obj;
            this.urlHash.put(crawlElement.getUrl().toString(), crawlElement);
        }
  }

    // TODO: Convert the collection to linked list or stack. (To do Depth First)
    /**
     * This class asssumes that merchant is already set.
     */
    public void setSite (Site site)
    {
        this.site = site;
        log.info ("======== Settting session site ==============");

        for (Group group : site.getGroupList()) {
            this.groupMap.put(group.getName(), group);
        }

        // Set Super's Site url
        try {
            super.setSiteUrl(IOUtil.getUrl(this.site.getUrl()));
        }
        catch (Exception ex) {
            log.info("======== Error Setting session's Site Url ==============");
            log.debug(ex.getMessage());
            log.debug(ex.getStackTrace());
        }

        // If there's no crawl elements, then lets create some.
        if (super.getToCrawlElements().size() == 0
                && super.getCrawledElements().size() == 0) {
            try {
                // Build the Root ToCrawlElement - From CategoryCrawl
                URL siteUrl = IOUtil.getUrl(site.getUrl());
                super.setSiteUrl(siteUrl);
                
                // Set up a new Crawlelement
                String groupName = site.getCategoryCrawl().getGroup();
                this.addCrawlElement(site.getCategoryCrawl().getUrl(), siteUrl, groupName, null);
               
                super.setErrorCount(0);
                super.setSuccessCount(0);
                
                // Init the crawl type.
                super.setCrawlType(CrawlManagerImpl.CRAWL_TYPE_CATEGORY);
                this.setIsRunning(false);             
                
                // Load the url hash.
                this.loadUrlHash();         // Load the url hash for testing url
            }
            catch (Exception ex) {
                log.debug("======== Error setting Site ==============");
                log.debug(ex.getMessage());
                log.debug(ex.getStackTrace());
            }
        }
    }
    
    /**
     * Reset the session to default, starting at one url
     */
    public void Reset() {
        super.getCrawledElements().clear();
        super.getToCrawlElements().clear();
        
        this.urlHash = new HashMap <String, CrawlElement>();
        this.setSite(this.getSite());           // This will cause the session to rest
    }

    /**
     * The site/crawl config for the current session
     * @return
     *      XSD site, do not confuse this with getSiteUrl.
     */
    public Site getSite()
    {
       return this.site;
    }

    /**
     * Group group info with in a site
     * 
     * @param GroupName 
     *          Name of the Group
     * @return Group 
     *          Null if there's no group found, or group if found.
     */
    public Group getGroup (String GroupName)
    {
        if (GroupName == null) {
            return null;
        }
        return this.groupMap.get(GroupName);
    }

    /**
     * Add a crawl element to list
     * 
     * @param RelativeUrl
     *          Relative url string
     * @param AbsolteURL
     *          Absolute Url, also known as context in java.
     * @param groupName
     *          Name of the group the crawl element belongs to
     * @return
     *          Newly created crawl element.
     */
    public CrawlElement addCrawlElement (
            String RelativeUrl,
            URL AbsolteURL,
            String groupName,
            ProductCategory pc)
    {
        
        URL url = null;
        try {
            url = new URL(AbsolteURL, RelativeUrl);
        }
        catch (Exception ex) {
            url = null;
        }        
        
        if (url != null && !(this.urlHash.containsKey(url.toString()))) {
            CrawlElement crawlElement = CrawlElement.Factory.newInstance();
            crawlElement.setUrl(url);
            crawlElement.setElement(groupName);
            
            crawlElement.setCategory(pc);     
            
            crawlElement.setCrawledSession(null);          
            crawlElement.setToCrawlSession(this);
            
            // Call DAO and create the crawl element.
            //super.getCrawlElmentDao().create(crawlElement);            
            
            super.getToCrawlElements().add(crawlElement);                   // Add to toCrawlList
            
            this.urlHash.put(url.toString(), crawlElement);          // Add to the urlHashlist     
            
            log.debug("New Crawl Element added: " + url.toString());
            
            return crawlElement;
        }
        else {
            if (url == null) {
                log.debug("Crawl Element Failed to add (invalid url): " + RelativeUrl);                     
            }
            else {
                log.debug("Crawl Element Failed to add (already exits): " + RelativeUrl);                 
            }
            return null;               
        }
    }
    
    /**
     * Get the next Crawl Element in the stack to Scrape
     */
    public CrawlElement getCrawlElement ()
    {
        if (super.getToCrawlElements().size() == 0) {
            return null;
        }

        // TODO: Implement depth first (Hibernate uses set, which return random object in list)
        for (Object obj : super.getToCrawlElements()) {            
            CrawlElement crawlElement = (CrawlElement) obj;

            super.getCrawledElements().add(crawlElement);
            crawlElement.setCrawledSession(this);
            
            super.getToCrawlElements().remove(crawlElement);   
            crawlElement.setToCrawlSession(null);

            return (CrawlElement) obj;
        }

        return null; // Should enver get here
    }

    /**
     * Get the Total URL count
     * 
     * @return int Total URL count (Scraped + Not Scraped)
     */
    public int getTotalURLCount ()
    {
        return super.getToCrawlElements().size() + super.getCrawledElements().size();
    }

    /**
     * Return the Processed Url count
     * @return
     */
    public int getProcessedUrlCount ()
    {
        return super.getCrawledElements().size();
    }

    /**
     * Return remaining url count
     * @return
     */
    public int getEstimatedRemainingUrlCount ()
    {
        return super.getToCrawlElements().size();
    }

    public void dummy ()
    {
 
    }

    /***
     * This function is called by CrawlTask to indicate this session is running
     * @param value
     */
    public void setIsRunning (boolean value)
    {
        this.isRunning = value;
    }

    /**
     * Out put the runnign status on screen
     * @return
     */
    public boolean getIsRunning ()
    {
        return this.isRunning;
    }
    
    /**
     * Debug string: (Category tree + product) in text
     * @return
     */
    public String getDebugString() {
        return debugString;
    }

    /**
     * Also called by crawltask to set the debug string.
     * @return
     */
    public void setDebugString(String value) {
        debugString = value;
    }
    
    /**
     * Get the current phase
     * @return
     *      Current Phase description (Downlading, Converting, Extracting, Finished...")
     */
    public String getPhase() {
        return this.phase;        
    }
    
    /**
     * Also called by crawltask to set the debug string.
     * @return
     */
    public void setPhase(String value) {
        System.out.println("Phase: " + value);
        this.phase = value;
    }
    
    public boolean getIsAbort() {
        return this.isAbort;
    }
    
    public void setIsAbort(boolean value) {
        this.isAbort = value;
    }
    
    /**
     * Overloads equals
     */
    public boolean equals(Object o) {
        if (o == null) 
            return false;
        
        if (o instanceof CrawlSession) 
        {
            log.debug("Testing Crawl Session Equals.");         // Compare the two site urls
            return (((CrawlSession) o).getSiteUrl().toString().equals(this.getSiteUrl().toString()));
        }
        return false;
    }
    
    public int hashCode() {
        return 0;
    }
    
    Log log = Log.getInstance(CrawlSessionImpl.class);
}